{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Pipeline for Image Preparation - FINAL FIXED VERSION\n",
    "\n",
    "## Key Fixes in This Version:\n",
    "\n",
    "### Step 6 - Continuous Global Numbering:\n",
    "- Numbering continues across ALL images (not restarting for each image)\n",
    "- Format: `00000001_filename.png` (8-digit index at the beginning)\n",
    "\n",
    "### Step 7 - FIXED Black Detection and File Handling:\n",
    "- **Cleans old format files** first (removes any leftover files from previous runs)\n",
    "- **Properly detects black masks** by checking if all RGB values are (0,0,0)\n",
    "- **Only processes new format files** (8-digit prefix)\n",
    "- **Two-phase cleaning**:\n",
    "  1. Remove completely black masks\n",
    "  2. Remove non-matching snippets from orthos/normalmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration Cell - Set All Paths and Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\images\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\masks\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\heightmaps\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\normalmaps\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\snippets_orthomosaics\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\snippets_masks\n",
      "‚úì Directory ready: C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\\snippets_normalmaps\n",
      "\n",
      "‚úÖ All directories created/verified!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === MAIN CONFIGURATION ===\n",
    "# Base directory containing your project\n",
    "BASE_DIR = \"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX\"\n",
    "\n",
    "# Input paths\n",
    "COCO_JSON_PATH = os.path.join(BASE_DIR, \"2025-08-10.json\")\n",
    "ORIGINAL_IMAGES_DIR = os.path.join(BASE_DIR, \"images\")  # Contains full-res orthomosaics\n",
    "\n",
    "# Full-sized data directories\n",
    "FULL_ORTHOMOSAICS_DIR = ORIGINAL_IMAGES_DIR  # Same as images\n",
    "FULL_MASKS_DIR = os.path.join(BASE_DIR, \"masks\")\n",
    "FULL_HEIGHTMAPS_DIR = os.path.join(BASE_DIR, \"heightmaps\")\n",
    "FULL_NORMALMAPS_DIR = os.path.join(BASE_DIR, \"normalmaps\")\n",
    "\n",
    "# Snippet directories\n",
    "SNIPPET_ORTHOMOSAICS_DIR = os.path.join(BASE_DIR, \"snippets_orthomosaics\")\n",
    "SNIPPET_MASKS_DIR = os.path.join(BASE_DIR, \"snippets_masks\")\n",
    "SNIPPET_NORMALMAPS_DIR = os.path.join(BASE_DIR, \"snippets_normalmaps\")\n",
    "\n",
    "# Parameters\n",
    "CROP_SIZE = 1280  # Size of the snippets\n",
    "DESIRED_COVERAGE = 1.6  # Coverage factor for Sobol splitting (1.6 = 160% coverage)\n",
    "BLACK_THRESHOLD = 1.0  # Threshold for excluding black images (1.0 = 100% black pixels)\n",
    "EPS = 0.001  # Scaling factor for normal map computation\n",
    "\n",
    "# Class colors for COCO mask generation\n",
    "CLASS_COLORS = {\n",
    "    1: (0, 0, 255),      # Class 1 - Blue \n",
    "    2: (255, 255, 0),    # Class 2 - Yellow\n",
    "    3: (255, 0, 0),      # Class 3 - Red\n",
    "    # 4: (0, 255, 0),    # Class 4 - Green\n",
    "}\n",
    "\n",
    "# GLOBAL INDEX FOR SOBOL SPLITTING (DO NOT MODIFY)\n",
    "GLOBAL_CROP_INDEX = 1\n",
    "\n",
    "# Create all directories if they don't exist\n",
    "directories = [\n",
    "    ORIGINAL_IMAGES_DIR, FULL_MASKS_DIR, FULL_HEIGHTMAPS_DIR, FULL_NORMALMAPS_DIR,\n",
    "    SNIPPET_ORTHOMOSAICS_DIR, SNIPPET_MASKS_DIR, SNIPPET_NORMALMAPS_DIR\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"‚úì Directory ready: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ All directories created/verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import All Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "import cv2\n",
    "import math\n",
    "import sobol_seq\n",
    "import rasterio\n",
    "import glob\n",
    "\n",
    "# Disable PIL image size limit\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Export COCO JSON to Mask Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_coco_json_to_masks():\n",
    "    \"\"\"Export COCO JSON annotations to colored mask images.\"\"\"\n",
    "    print(\"\\n=== Step 3: Exporting COCO JSON to Masks ===\")\n",
    "    \n",
    "    # Load COCO JSON\n",
    "    with open(COCO_JSON_PATH) as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # COCO API object\n",
    "    coco = COCO(COCO_JSON_PATH)\n",
    "    \n",
    "    # Get all image IDs\n",
    "    image_ids = coco.getImgIds()\n",
    "    \n",
    "    # Process each image - SORTED for consistency\n",
    "    for i, image_id in enumerate(sorted(image_ids)):\n",
    "        # Load image data\n",
    "        img_data = coco.loadImgs(image_id)[0]\n",
    "        img_filename = img_data['file_name']\n",
    "        \n",
    "        # Load annotations for this image\n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Create empty mask\n",
    "        img_width = img_data['width']\n",
    "        img_height = img_data['height']\n",
    "        mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Process each annotation\n",
    "        for ann in annotations:\n",
    "            # Get mask for this annotation\n",
    "            if isinstance(ann['segmentation'], list):  # Polygon\n",
    "                seg_mask = coco.annToMask(ann)\n",
    "            else:  # RLE\n",
    "                rle = ann['segmentation']\n",
    "                seg_mask = coco_mask.decode(rle)\n",
    "            \n",
    "            # Get class color\n",
    "            class_id = ann['category_id']\n",
    "            color = CLASS_COLORS.get(class_id, (0, 0, 0))\n",
    "            \n",
    "            # Apply color to mask\n",
    "            mask[seg_mask == 1] = color\n",
    "        \n",
    "        # Save mask\n",
    "        mask_img = Image.fromarray(mask)\n",
    "        output_path = os.path.join(FULL_MASKS_DIR, img_filename.replace('.jpg', '_mask.png'))\n",
    "        mask_img.save(output_path)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(image_ids)} images...\")\n",
    "    \n",
    "    print(f\"‚úÖ Complete: {len(image_ids)} masks saved to {FULL_MASKS_DIR}\")\n",
    "\n",
    "# Run Step 3\n",
    "export_coco_json_to_masks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(a) Data Augmentation - Flip PNG Images (Orthomosaics & Masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4a: Flipping orthomosaics ===\n",
      "‚úÖ Flipped 12 orthomosaics\n",
      "\n",
      "=== Step 4a: Flipping masks ===\n",
      "‚úÖ Flipped 12 masks\n"
     ]
    }
   ],
   "source": [
    "def flip_png_images(input_folder, image_type=\"images\"):\n",
    "    \"\"\"Flip PNG images horizontally for data augmentation.\"\"\"\n",
    "    print(f\"\\n=== Step 4a: Flipping {image_type} ===\")\n",
    "    \n",
    "    processed = 0\n",
    "    # SORTED for consistency\n",
    "    for filename in sorted(os.listdir(input_folder)):\n",
    "        if filename.lower().endswith(\".png\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Skip if already flipped\n",
    "            if filename.startswith(\"flipped_\"):\n",
    "                continue\n",
    "            \n",
    "            # Open and flip image\n",
    "            with Image.open(file_path) as img:\n",
    "                flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Save flipped image\n",
    "            new_filename = \"flipped_\" + filename\n",
    "            output_path = os.path.join(input_folder, new_filename)\n",
    "            flipped_img.save(output_path)\n",
    "            processed += 1\n",
    "    \n",
    "    print(f\"‚úÖ Flipped {processed} {image_type}\")\n",
    "\n",
    "# Run flipping for orthomosaics and masks\n",
    "if os.path.exists(FULL_ORTHOMOSAICS_DIR):\n",
    "    flip_png_images(FULL_ORTHOMOSAICS_DIR, \"orthomosaics\")\n",
    "flip_png_images(FULL_MASKS_DIR, \"masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(b) Data Augmentation - Flip TIFF Images (Heightmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4b: Flipping Heightmaps ===\n",
      "‚úÖ Flipped 12 heightmaps\n"
     ]
    }
   ],
   "source": [
    "def flip_geotiffs(input_folder):\n",
    "    \"\"\"Flip GeoTIFF images horizontally for data augmentation.\"\"\"\n",
    "    print(f\"\\n=== Step 4b: Flipping Heightmaps ===\")\n",
    "    \n",
    "    processed = 0\n",
    "    # SORTED for consistency\n",
    "    for filename in sorted(os.listdir(input_folder)):\n",
    "        if filename.lower().endswith((\".tif\", \".tiff\")):\n",
    "            # Skip if already flipped\n",
    "            if filename.startswith(\"flipped_\"):\n",
    "                continue\n",
    "                \n",
    "            in_fp = os.path.join(input_folder, filename)\n",
    "            out_fp = os.path.join(input_folder, \"flipped_\" + filename)\n",
    "            \n",
    "            # Read with rasterio\n",
    "            with rasterio.open(in_fp) as src:\n",
    "                profile = src.profile.copy()\n",
    "                data = src.read()  # shape: (bands, height, width)\n",
    "            \n",
    "            # Flip horizontally\n",
    "            flipped_data = np.flip(data, axis=2)\n",
    "            \n",
    "            # Write flipped data\n",
    "            with rasterio.open(out_fp, 'w', **profile) as dst:\n",
    "                dst.write(flipped_data)\n",
    "            \n",
    "            processed += 1\n",
    "    \n",
    "    print(f\"‚úÖ Flipped {processed} heightmaps\")\n",
    "\n",
    "# Run flipping for heightmaps if directory exists\n",
    "if os.path.exists(FULL_HEIGHTMAPS_DIR):\n",
    "    flip_geotiffs(FULL_HEIGHTMAPS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convert Heightmaps to Normal Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 5: Converting Heightmaps to Normal Maps ===\n",
      "  Processed 1/24 heightmaps...\n",
      "  Processed 6/24 heightmaps...\n",
      "  Processed 11/24 heightmaps...\n",
      "  Processed 16/24 heightmaps...\n",
      "  Processed 21/24 heightmaps...\n",
      "‚úÖ Converted 24 heightmaps to normal maps\n"
     ]
    }
   ],
   "source": [
    "def load_hdr_image(input_path):\n",
    "    \"\"\"Load heightmap from TIFF file.\"\"\"\n",
    "    with rasterio.open(input_path) as src:\n",
    "        dem = src.read(1)\n",
    "    return dem\n",
    "\n",
    "def compute_normals(dem, eps):\n",
    "    \"\"\"Compute normal map from heightmap.\"\"\"\n",
    "    dzdx = np.gradient(dem, axis=1)  # Derivative in x-direction\n",
    "    dzdy = np.gradient(dem, axis=0)  # Derivative in y-direction\n",
    "    normals = np.dstack((dzdx, dzdy, eps * np.ones_like(dem)))\n",
    "    norm = np.linalg.norm(normals, axis=2, keepdims=True)\n",
    "    return normals / norm\n",
    "\n",
    "def save_normal_map_png(normals, output_path):\n",
    "    \"\"\"Save normal map as PNG.\"\"\"\n",
    "    normals_scaled = (normals + 1) / 2\n",
    "    normals_scaled = np.clip(normals_scaled, 0, 1)\n",
    "    normals_8bit = (normals_scaled * 255).astype(np.uint8)\n",
    "    image = Image.fromarray(normals_8bit)\n",
    "    image.save(output_path, format=\"PNG\")\n",
    "\n",
    "def process_heightmaps_to_normals():\n",
    "    \"\"\"Convert all heightmaps to normal maps.\"\"\"\n",
    "    print(f\"\\n=== Step 5: Converting Heightmaps to Normal Maps ===\")\n",
    "    \n",
    "    if not os.path.exists(FULL_HEIGHTMAPS_DIR):\n",
    "        print(\"  No heightmaps directory found, skipping normal map generation\")\n",
    "        return\n",
    "    \n",
    "    pattern = os.path.join(FULL_HEIGHTMAPS_DIR, '*.tif')\n",
    "    files = sorted(glob.glob(pattern))  # SORTED for consistency\n",
    "    \n",
    "    if not files:\n",
    "        print(\"  No .tif files found in heightmaps directory\")\n",
    "        return\n",
    "    \n",
    "    for i, inp in enumerate(files):\n",
    "        base = os.path.splitext(os.path.basename(inp))[0]\n",
    "        outp = os.path.join(FULL_NORMALMAPS_DIR, f\"{base}_normalmap.png\")\n",
    "        \n",
    "        # Process heightmap to normal map\n",
    "        dem = load_hdr_image(inp)\n",
    "        normals = compute_normals(dem, EPS)\n",
    "        save_normal_map_png(normals, outp)\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(files)} heightmaps...\")\n",
    "    \n",
    "    print(f\"‚úÖ Converted {len(files)} heightmaps to normal maps\")\n",
    "\n",
    "# Run heightmap to normal conversion\n",
    "process_heightmaps_to_normals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sobol Splitting - WITH CONTINUOUS GLOBAL NUMBERING\n",
    "\n",
    "**The numbering continues across ALL images**:\n",
    "- Image 1: crops 00000001-00000345\n",
    "- Image 2: crops 00000346-00000690\n",
    "- Image 3: crops 00000691-00001234\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: SOBOL SPLITTING WITH CONTINUOUS GLOBAL NUMBERING\n",
      "================================================================================\n",
      "  Global index counter reset to 1\n",
      "\n",
      "=== Step 6: Sobol Splitting orthomosaics ===\n",
      "  Starting from index: 00000001\n",
      "  Processed: H_Bf_1-4_png-ortho.png ‚Üí 734 crops (indices 00000001-00000734)\n",
      "  Processed: H_Bf_15a_png-ortho.png ‚Üí 8 crops (indices 00000735-00000742)\n",
      "  Processed: H_Bf_15b_png-ortho.png ‚Üí 7 crops (indices 00000743-00000749)\n",
      "  Processed: H_Bf_15c_png-ortho.png ‚Üí 8 crops (indices 00000750-00000757)\n",
      "  Processed: H_Bf_16-19_png-ortho.png ‚Üí 543 crops (indices 00000758-00001300)\n",
      "  Processed: H_Bf_22_png-ortho.png ‚Üí 158 crops (indices 00001301-00001458)\n",
      "  Processed: H_Bf_28_png-ortho.png ‚Üí 228 crops (indices 00001459-00001686)\n",
      "  Processed: H_Bf_38-42_png-ortho.png ‚Üí 91 crops (indices 00001687-00001777)\n",
      "  Processed: H_Bf_38-42d_png-ortho.png ‚Üí 91 crops (indices 00001778-00001868)\n",
      "  Processed: H_Bf_9-14_png-ortho.png ‚Üí 345 crops (indices 00001869-00002213)\n",
      "  Processed: K_Bf_13c_png-ortho.png ‚Üí 250 crops (indices 00002214-00002463)\n",
      "  Processed: K_Bf_18-21_png-ortho.png ‚Üí 31 crops (indices 00002464-00002494)\n",
      "  Processed: flipped_H_Bf_1-4_png-ortho.png ‚Üí 734 crops (indices 00002495-00003228)\n",
      "  Processed: flipped_H_Bf_15a_png-ortho.png ‚Üí 8 crops (indices 00003229-00003236)\n",
      "  Processed: flipped_H_Bf_15b_png-ortho.png ‚Üí 7 crops (indices 00003237-00003243)\n",
      "  Processed: flipped_H_Bf_15c_png-ortho.png ‚Üí 8 crops (indices 00003244-00003251)\n",
      "  Processed: flipped_H_Bf_16-19_png-ortho.png ‚Üí 543 crops (indices 00003252-00003794)\n",
      "  Processed: flipped_H_Bf_22_png-ortho.png ‚Üí 158 crops (indices 00003795-00003952)\n",
      "  Processed: flipped_H_Bf_28_png-ortho.png ‚Üí 228 crops (indices 00003953-00004180)\n",
      "  Processed: flipped_H_Bf_38-42_png-ortho.png ‚Üí 91 crops (indices 00004181-00004271)\n",
      "  Processed: flipped_H_Bf_38-42d_png-ortho.png ‚Üí 91 crops (indices 00004272-00004362)\n",
      "  Processed: flipped_H_Bf_9-14_png-ortho.png ‚Üí 345 crops (indices 00004363-00004707)\n",
      "  Processed: flipped_K_Bf_13c_png-ortho.png ‚Üí 250 crops (indices 00004708-00004957)\n",
      "  Processed: flipped_K_Bf_18-21_png-ortho.png ‚Üí 31 crops (indices 00004958-00004988)\n",
      "‚úÖ orthomosaics: Processed 24 images, created 4988 snippets\n",
      "  Final index reached: 00004988\n",
      "  Global index counter reset to 1\n",
      "\n",
      "=== Step 6: Sobol Splitting masks ===\n",
      "  Starting from index: 00000001\n",
      "  Processed: H_Bf_1-4_png-ortho.png ‚Üí 734 crops (indices 00000001-00000734)\n",
      "  Processed: H_Bf_15a_Ortho_mask.png ‚Üí 8 crops (indices 00000735-00000742)\n",
      "  Processed: H_Bf_15b_Ortho_mask.png ‚Üí 7 crops (indices 00000743-00000749)\n",
      "  Processed: H_Bf_15c_Ortho_mask.png ‚Üí 8 crops (indices 00000750-00000757)\n",
      "  Processed: H_Bf_16-19_png-ortho.png ‚Üí 543 crops (indices 00000758-00001300)\n",
      "  Processed: H_Bf_22_png-ortho.png ‚Üí 158 crops (indices 00001301-00001458)\n",
      "  Processed: H_Bf_28_Ortho_mask.png ‚Üí 228 crops (indices 00001459-00001686)\n",
      "  Processed: H_Bf_38-42_png-ortho.png ‚Üí 91 crops (indices 00001687-00001777)\n",
      "  Processed: H_Bf_38-42d_png-ortho.png ‚Üí 91 crops (indices 00001778-00001868)\n",
      "  Processed: H_Bf_9-14_png-ortho.png ‚Üí 345 crops (indices 00001869-00002213)\n",
      "  Processed: K_Bf_13c_png-ortho.png ‚Üí 250 crops (indices 00002214-00002463)\n",
      "  Processed: K_Bf_18-21_png-ortho.png ‚Üí 31 crops (indices 00002464-00002494)\n",
      "  Processed: flipped_H_Bf_1-4_png-ortho.png ‚Üí 734 crops (indices 00002495-00003228)\n",
      "  Processed: flipped_H_Bf_15a_Ortho_mask.png ‚Üí 8 crops (indices 00003229-00003236)\n",
      "  Processed: flipped_H_Bf_15b_Ortho_mask.png ‚Üí 7 crops (indices 00003237-00003243)\n",
      "  Processed: flipped_H_Bf_15c_Ortho_mask.png ‚Üí 8 crops (indices 00003244-00003251)\n",
      "  Processed: flipped_H_Bf_16-19_png-ortho.png ‚Üí 543 crops (indices 00003252-00003794)\n",
      "  Processed: flipped_H_Bf_22_png-ortho.png ‚Üí 158 crops (indices 00003795-00003952)\n",
      "  Processed: flipped_H_Bf_28_Ortho_mask.png ‚Üí 228 crops (indices 00003953-00004180)\n",
      "  Processed: flipped_H_Bf_38-42_png-ortho.png ‚Üí 91 crops (indices 00004181-00004271)\n",
      "  Processed: flipped_H_Bf_38-42d_png-ortho.png ‚Üí 91 crops (indices 00004272-00004362)\n",
      "  Processed: flipped_H_Bf_9-14_png-ortho.png ‚Üí 345 crops (indices 00004363-00004707)\n",
      "  Processed: flipped_K_Bf_13c_png-ortho.png ‚Üí 250 crops (indices 00004708-00004957)\n",
      "  Processed: flipped_K_Bf_18-21_png-ortho.png ‚Üí 31 crops (indices 00004958-00004988)\n",
      "‚úÖ masks: Processed 24 images, created 4988 snippets\n",
      "  Final index reached: 00004988\n",
      "  Global index counter reset to 1\n",
      "\n",
      "=== Step 6: Sobol Splitting normalmaps ===\n",
      "  Starting from index: 00000001\n",
      "  Processed: H_Bf_1-4_DEM_normalmap.png ‚Üí 734 crops (indices 00000001-00000734)\n",
      "  Processed: H_Bf_15a_DEM_normalmap.png ‚Üí 8 crops (indices 00000735-00000742)\n",
      "  Processed: H_Bf_15b_DEM_normalmap.png ‚Üí 7 crops (indices 00000743-00000749)\n",
      "  Processed: H_Bf_15c_DEM_normalmap.png ‚Üí 8 crops (indices 00000750-00000757)\n",
      "  Processed: H_Bf_16-19_DEM_normalmap.png ‚Üí 543 crops (indices 00000758-00001300)\n",
      "  Processed: H_Bf_22_DEM_normalmap.png ‚Üí 158 crops (indices 00001301-00001458)\n",
      "  Processed: H_Bf_28_DEM_normalmap.png ‚Üí 228 crops (indices 00001459-00001686)\n",
      "  Processed: H_Bf_38-42_DEM_normalmap.png ‚Üí 91 crops (indices 00001687-00001777)\n",
      "  Processed: H_Bf_38-42d_DEM_normalmap.png ‚Üí 91 crops (indices 00001778-00001868)\n",
      "  Processed: H_Bf_9-14_DEM_normalmap.png ‚Üí 345 crops (indices 00001869-00002213)\n",
      "  Processed: K_Bf_13c_DEM_normalmap.png ‚Üí 250 crops (indices 00002214-00002463)\n",
      "  Processed: K_Bf_18-21_DEM_normalmap.png ‚Üí 31 crops (indices 00002464-00002494)\n",
      "  Processed: flipped_H_Bf_1-4_DEM_normalmap.png ‚Üí 734 crops (indices 00002495-00003228)\n",
      "  Processed: flipped_H_Bf_15a_DEM_normalmap.png ‚Üí 8 crops (indices 00003229-00003236)\n",
      "  Processed: flipped_H_Bf_15b_DEM_normalmap.png ‚Üí 7 crops (indices 00003237-00003243)\n",
      "  Processed: flipped_H_Bf_15c_DEM_normalmap.png ‚Üí 8 crops (indices 00003244-00003251)\n",
      "  Processed: flipped_H_Bf_16-19_DEM_normalmap.png ‚Üí 543 crops (indices 00003252-00003794)\n",
      "  Processed: flipped_H_Bf_22_DEM_normalmap.png ‚Üí 158 crops (indices 00003795-00003952)\n",
      "  Processed: flipped_H_Bf_28_DEM_normalmap.png ‚Üí 228 crops (indices 00003953-00004180)\n",
      "  Processed: flipped_H_Bf_38-42_DEM_normalmap.png ‚Üí 91 crops (indices 00004181-00004271)\n",
      "  Processed: flipped_H_Bf_38-42d_DEM_normalmap.png ‚Üí 91 crops (indices 00004272-00004362)\n",
      "  Processed: flipped_H_Bf_9-14_DEM_normalmap.png ‚Üí 345 crops (indices 00004363-00004707)\n",
      "  Processed: flipped_K_Bf_13c_DEM_normalmap.png ‚Üí 250 crops (indices 00004708-00004957)\n",
      "  Processed: flipped_K_Bf_18-21_DEM_normalmap.png ‚Üí 31 crops (indices 00004958-00004988)\n",
      "‚úÖ normalmaps: Processed 24 images, created 4988 snippets\n",
      "  Final index reached: 00004988\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: Snippet counts BEFORE cleaning\n",
      "================================================================================\n",
      "  ortho: 4988 snippets\n",
      "  mask: 4988 snippets\n",
      "  normal: 4988 snippets\n",
      "\n",
      "‚ö†Ô∏è  These counts should be identical at this stage.\n",
      "  Next step will remove black masks and matching orthos/normals.\n"
     ]
    }
   ],
   "source": [
    "def reset_global_index():\n",
    "    \"\"\"Reset the global index counter before starting the splitting process.\"\"\"\n",
    "    global GLOBAL_CROP_INDEX\n",
    "    GLOBAL_CROP_INDEX = 1\n",
    "    print(f\"  Global index counter reset to 1\")\n",
    "\n",
    "def sobol_split_images_continuous(input_folder, output_folder, data_type=\"images\"):\n",
    "    \"\"\"\n",
    "    Split images using Sobol sequence sampling with CONTINUOUS global numbering.\n",
    "    The index continues across ALL images, not restarting for each image.\n",
    "    \"\"\"\n",
    "    global GLOBAL_CROP_INDEX  # Use the global counter\n",
    "    \n",
    "    print(f\"\\n=== Step 6: Sobol Splitting {data_type} ===\")\n",
    "    print(f\"  Starting from index: {GLOBAL_CROP_INDEX:08d}\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_saved = 0\n",
    "    \n",
    "    # CRITICAL: Sort files to ensure consistent processing order\n",
    "    files_to_process = sorted([f for f in os.listdir(input_folder) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))])\n",
    "    \n",
    "    for file_idx, file in enumerate(files_to_process):\n",
    "        image_path = os.path.join(input_folder, file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"  Error loading {file}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        image_height, image_width = image.shape[:2]\n",
    "        if image_width < CROP_SIZE or image_height < CROP_SIZE:\n",
    "            print(f\"  {file} is smaller than crop size {CROP_SIZE}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate number of Sobol points\n",
    "        image_area = image_width * image_height\n",
    "        crop_area = CROP_SIZE * CROP_SIZE\n",
    "        num_points = math.ceil((DESIRED_COVERAGE * image_area) / crop_area)\n",
    "        \n",
    "        # Generate Sobol points\n",
    "        sobol_points = sobol_seq.i4_sobol_generate(2, num_points)\n",
    "        sobol_points[:, 0] = (sobol_points[:, 0] * (image_width - CROP_SIZE)).astype(int)\n",
    "        sobol_points[:, 1] = (sobol_points[:, 1] * (image_height - CROP_SIZE)).astype(int)\n",
    "        \n",
    "        base_name, _ = os.path.splitext(file)\n",
    "        crops_from_this_image = 0\n",
    "        starting_index = GLOBAL_CROP_INDEX  # Remember where we started for this image\n",
    "        \n",
    "        # Save EVERY crop with CONTINUOUS global numbering\n",
    "        for (x, y) in sobol_points:\n",
    "            x, y = int(x), int(y)\n",
    "            cropped_image = image[y:y + CROP_SIZE, x:x + CROP_SIZE]\n",
    "            \n",
    "            # Save with CONTINUOUS global index\n",
    "            output_filename = f\"{GLOBAL_CROP_INDEX:08d}_{base_name}.png\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cv2.imwrite(output_path, cropped_image)\n",
    "            \n",
    "            total_saved += 1\n",
    "            crops_from_this_image += 1\n",
    "            GLOBAL_CROP_INDEX += 1  # INCREMENT THE GLOBAL COUNTER\n",
    "        \n",
    "        total_processed += 1\n",
    "        ending_index = GLOBAL_CROP_INDEX - 1  # Last index used for this image\n",
    "        print(f\"  Processed: {file} ‚Üí {crops_from_this_image} crops (indices {starting_index:08d}-{ending_index:08d})\")\n",
    "    \n",
    "    print(f\"‚úÖ {data_type}: Processed {total_processed} images, created {total_saved} snippets\")\n",
    "    print(f\"  Final index reached: {GLOBAL_CROP_INDEX - 1:08d}\")\n",
    "    return total_saved\n",
    "\n",
    "# Run Sobol splitting for all three types\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: SOBOL SPLITTING WITH CONTINUOUS GLOBAL NUMBERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "counts = {}\n",
    "\n",
    "# Process orthomosaics\n",
    "reset_global_index()  # Start from 1\n",
    "if os.path.exists(FULL_ORTHOMOSAICS_DIR):\n",
    "    counts['ortho'] = sobol_split_images_continuous(\n",
    "        FULL_ORTHOMOSAICS_DIR, \n",
    "        SNIPPET_ORTHOMOSAICS_DIR, \n",
    "        \"orthomosaics\"\n",
    "    )\n",
    "\n",
    "# Process masks - RESET INDEX TO START FROM 1 AGAIN\n",
    "reset_global_index()  # Start from 1\n",
    "counts['mask'] = sobol_split_images_continuous(\n",
    "    FULL_MASKS_DIR, \n",
    "    SNIPPET_MASKS_DIR, \n",
    "    \"masks\"\n",
    ")\n",
    "\n",
    "# Process normalmaps - RESET INDEX TO START FROM 1 AGAIN\n",
    "reset_global_index()  # Start from 1\n",
    "if os.path.exists(FULL_NORMALMAPS_DIR):\n",
    "    counts['normal'] = sobol_split_images_continuous(\n",
    "        FULL_NORMALMAPS_DIR, \n",
    "        SNIPPET_NORMALMAPS_DIR, \n",
    "        \"normalmaps\"\n",
    "    )\n",
    "\n",
    "# Verification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: Snippet counts BEFORE cleaning\")\n",
    "print(\"=\"*80)\n",
    "for key, count in counts.items():\n",
    "    print(f\"  {key}: {count} snippets\")\n",
    "print(\"\\n‚ö†Ô∏è  These counts should be identical at this stage.\")\n",
    "print(\"  Next step will remove black masks and matching orthos/normals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clean Snippets - Remove Black Masks and Non-Matching Files\n",
    "\n",
    "This step:\n",
    "1. First removes any old-format files from previous runs\n",
    "2. Detects and removes completely black mask snippets\n",
    "3. Removes corresponding snippets from orthomosaics and normalmaps directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 7: Cleaning Snippets ===\n",
      "\n",
      "üìÅ Phase 1: Cleaning old-format files...\n",
      "\n",
      "üîç Phase 2: Finding black masks...\n",
      "  Found 2839 completely black masks out of 4988 total masks\n",
      "\n",
      "üóëÔ∏è  Phase 3: Removing black masks and corresponding files...\n",
      "  Removed 2839 black masks\n",
      "  Removed 2839 corresponding orthomosaics\n",
      "  Removed 2839 corresponding normalmaps\n",
      "\n",
      "‚úÖ Phase 4: Final verification...\n",
      "\n",
      "Final snippet counts after cleaning:\n",
      "  masks: 2149 snippets\n",
      "  orthos: 2149 snippets\n",
      "  normals: 2149 snippets\n",
      "\n",
      "‚úÖ SUCCESS: All directories have the same number of snippets!\n"
     ]
    }
   ],
   "source": [
    "def clean_old_format_files(directory):\n",
    "    \"\"\"Remove any files that don't follow the 8-digit prefix format.\"\"\"\n",
    "    import re\n",
    "    pattern = re.compile(r'^\\d{8}_.*\\.png$')\n",
    "    removed = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if not pattern.match(filename):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                removed += 1\n",
    "    \n",
    "    if removed > 0:\n",
    "        print(f\"  Removed {removed} old-format files from {directory}\")\n",
    "    return removed\n",
    "\n",
    "def is_completely_black(image_path):\n",
    "    \"\"\"Check if an image is completely black (all pixels are [0,0,0]).\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return False\n",
    "    \n",
    "    # Check if all pixels are exactly [0,0,0] in BGR\n",
    "    return np.all(image == 0)\n",
    "\n",
    "def extract_index_from_filename(filename):\n",
    "    \"\"\"Extract the 8-digit index from filename like '00000123_image.png'.\"\"\"\n",
    "    if len(filename) >= 8 and filename[:8].isdigit():\n",
    "        return filename[:8]\n",
    "    return None\n",
    "\n",
    "def remove_black_masks_and_sync():\n",
    "    \"\"\"Remove black masks and synchronize all snippet directories.\"\"\"\n",
    "    print(\"\\n=== Step 7: Cleaning Snippets ===\")\n",
    "    \n",
    "    # Phase 1: Clean old format files\n",
    "    print(\"\\nüìÅ Phase 1: Cleaning old-format files...\")\n",
    "    directories = [SNIPPET_MASKS_DIR, SNIPPET_ORTHOMOSAICS_DIR]\n",
    "    if os.path.exists(SNIPPET_NORMALMAPS_DIR):\n",
    "        directories.append(SNIPPET_NORMALMAPS_DIR)\n",
    "    \n",
    "    for directory in directories:\n",
    "        clean_old_format_files(directory)\n",
    "    \n",
    "    # Phase 2: Find and remove black masks\n",
    "    print(\"\\nüîç Phase 2: Finding black masks...\")\n",
    "    black_mask_indices = set()\n",
    "    total_masks = 0\n",
    "    \n",
    "    for filename in sorted(os.listdir(SNIPPET_MASKS_DIR)):\n",
    "        if filename.endswith('.png'):\n",
    "            total_masks += 1\n",
    "            file_path = os.path.join(SNIPPET_MASKS_DIR, filename)\n",
    "            \n",
    "            if is_completely_black(file_path):\n",
    "                index = extract_index_from_filename(filename)\n",
    "                if index:\n",
    "                    black_mask_indices.add(index)\n",
    "    \n",
    "    print(f\"  Found {len(black_mask_indices)} completely black masks out of {total_masks} total masks\")\n",
    "    \n",
    "    # Phase 3: Remove black masks and corresponding files\n",
    "    print(\"\\nüóëÔ∏è  Phase 3: Removing black masks and corresponding files...\")\n",
    "    \n",
    "    removed_counts = {'masks': 0, 'orthos': 0, 'normals': 0}\n",
    "    \n",
    "    # Remove from masks directory\n",
    "    for filename in os.listdir(SNIPPET_MASKS_DIR):\n",
    "        index = extract_index_from_filename(filename)\n",
    "        if index in black_mask_indices:\n",
    "            file_path = os.path.join(SNIPPET_MASKS_DIR, filename)\n",
    "            os.remove(file_path)\n",
    "            removed_counts['masks'] += 1\n",
    "    \n",
    "    # Remove from orthomosaics directory\n",
    "    if os.path.exists(SNIPPET_ORTHOMOSAICS_DIR):\n",
    "        for filename in os.listdir(SNIPPET_ORTHOMOSAICS_DIR):\n",
    "            index = extract_index_from_filename(filename)\n",
    "            if index in black_mask_indices:\n",
    "                file_path = os.path.join(SNIPPET_ORTHOMOSAICS_DIR, filename)\n",
    "                os.remove(file_path)\n",
    "                removed_counts['orthos'] += 1\n",
    "    \n",
    "    # Remove from normalmaps directory\n",
    "    if os.path.exists(SNIPPET_NORMALMAPS_DIR):\n",
    "        for filename in os.listdir(SNIPPET_NORMALMAPS_DIR):\n",
    "            index = extract_index_from_filename(filename)\n",
    "            if index in black_mask_indices:\n",
    "                file_path = os.path.join(SNIPPET_NORMALMAPS_DIR, filename)\n",
    "                os.remove(file_path)\n",
    "                removed_counts['normals'] += 1\n",
    "    \n",
    "    print(f\"  Removed {removed_counts['masks']} black masks\")\n",
    "    print(f\"  Removed {removed_counts['orthos']} corresponding orthomosaics\")\n",
    "    print(f\"  Removed {removed_counts['normals']} corresponding normalmaps\")\n",
    "    \n",
    "    # Phase 4: Final verification\n",
    "    print(\"\\n‚úÖ Phase 4: Final verification...\")\n",
    "    final_counts = {}\n",
    "    final_counts['masks'] = len([f for f in os.listdir(SNIPPET_MASKS_DIR) if f.endswith('.png')])\n",
    "    if os.path.exists(SNIPPET_ORTHOMOSAICS_DIR):\n",
    "        final_counts['orthos'] = len([f for f in os.listdir(SNIPPET_ORTHOMOSAICS_DIR) if f.endswith('.png')])\n",
    "    if os.path.exists(SNIPPET_NORMALMAPS_DIR):\n",
    "        final_counts['normals'] = len([f for f in os.listdir(SNIPPET_NORMALMAPS_DIR) if f.endswith('.png')])\n",
    "    \n",
    "    print(\"\\nFinal snippet counts after cleaning:\")\n",
    "    for key, count in final_counts.items():\n",
    "        print(f\"  {key}: {count} snippets\")\n",
    "    \n",
    "    # Check if all counts match\n",
    "    all_counts = list(final_counts.values())\n",
    "    if len(set(all_counts)) == 1:\n",
    "        print(\"\\n‚úÖ SUCCESS: All directories have the same number of snippets!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Snippet counts don't match! Manual verification needed.\")\n",
    "    \n",
    "    return final_counts\n",
    "\n",
    "# Run the cleaning process\n",
    "final_counts = remove_black_masks_and_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Directory Structure:\")\n",
    "print(f\"  Base Directory: {BASE_DIR}\")\n",
    "print(\"\\n  Full-sized data:\")\n",
    "print(f\"    ‚Ä¢ Orthomosaics: {FULL_ORTHOMOSAICS_DIR}\")\n",
    "print(f\"    ‚Ä¢ Masks: {FULL_MASKS_DIR}\")\n",
    "print(f\"    ‚Ä¢ Heightmaps: {FULL_HEIGHTMAPS_DIR}\")\n",
    "print(f\"    ‚Ä¢ Normal maps: {FULL_NORMALMAPS_DIR}\")\n",
    "print(\"\\n  Snippets (1280x1280):\")\n",
    "print(f\"    ‚Ä¢ Orthomosaics: {SNIPPET_ORTHOMOSAICS_DIR}\")\n",
    "print(f\"    ‚Ä¢ Masks: {SNIPPET_MASKS_DIR}\")\n",
    "print(f\"    ‚Ä¢ Normal maps: {SNIPPET_NORMALMAPS_DIR}\")\n",
    "\n",
    "print(\"\\nüìà Processing Parameters:\")\n",
    "print(f\"  ‚Ä¢ Crop size: {CROP_SIZE}x{CROP_SIZE} pixels\")\n",
    "print(f\"  ‚Ä¢ Coverage factor: {DESIRED_COVERAGE}x\")\n",
    "print(f\"  ‚Ä¢ Black threshold: {BLACK_THRESHOLD * 100}%\")\n",
    "\n",
    "print(\"\\n‚úÖ All steps completed successfully!\")\n",
    "print(\"\\nYour data is now ready for training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
