{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net CNN Model for Ancient Wall Segmentation (4 Classes) - Normal Maps Only\n",
    "## Classes:\n",
    "- 0: Background (Black)\n",
    "- 1: Ashlar (Blue)\n",
    "- 2: Quarry (Yellow)\n",
    "- 3: Polygonal (Red)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Python import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Setup device\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Enable TF32 for better performance\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Specify the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes\n",
    "CLASS_NAMES = ['Background', 'Ashlar', 'Polygonal', 'Quarry']\n",
    "CLASS_COLORS = ['black', 'blue', 'red', 'yellow']\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Data directories - Mac\n",
    "#normalsDirectory = \"/Users/nilsschnorr/Desktop/2025-03-12_Trainingsdaten/1280_normals\"\n",
    "#maskDirectory = \"/Users/nilsschnorr/Desktop/2025-03-12_Trainingsdaten/1280_masks\"\n",
    "\n",
    "# Data directories - Windows\n",
    "normalsDirectory = \"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/snippets_normalmaps\"\n",
    "maskDirectory = \"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/snippets_masks\"\n",
    "\n",
    "# Model parameters\n",
    "modelTrainedName = \"2025-08-11_3-channel_4-class-EX\"\n",
    "modelTrainedExt = \"pth\"\n",
    "numEpochs = 300\n",
    "debugModus = True\n",
    "\n",
    "modelTrainedName = modelTrainedName + \"_\" + str(numEpochs) + \".\" + modelTrainedExt\n",
    "\n",
    "print('=== 4-Class Segmentation Model (Normal Maps Only) ===')\n",
    "print('Classes:', CLASS_NAMES)\n",
    "print('Model name:', modelTrainedName)\n",
    "print('Number of epochs:', numEpochs)\n",
    "print('Input channels: 3 (Normal Maps)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Define the U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels, dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling with bilinear interpolation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv_after_up = nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, padding=1)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, dropout_rate)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x1 = self.conv_after_up(x1)\n",
    "        \n",
    "        # Handle size mismatch\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        if diffX != 0 or diffY != 0:\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                            diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class MultiUNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=4):\n",
    "        super(MultiUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_channels, 16, 0.1)\n",
    "        self.down1 = Down(16, 32, 0.1)\n",
    "        self.down2 = Down(32, 64, 0.2)\n",
    "        self.down3 = Down(64, 128, 0.2)\n",
    "        self.down4 = Down(128, 256, 0.3)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = Up(256, 128, 0.2)\n",
    "        self.up2 = Up(128, 64, 0.2)\n",
    "        self.up3 = Up(64, 32, 0.1)\n",
    "        self.up4 = Up(32, 16, 0.1)\n",
    "        self.outc = nn.Conv2d(16, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"Initialize model weights\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Load the input images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size\n",
    "SIZE_X, SIZE_Y = 512, 512\n",
    "\n",
    "# Get sorted lists of files\n",
    "normal_paths = sorted(glob.glob(os.path.join(normalsDirectory, \"*.png\")))\n",
    "\n",
    "# Load training images (Normal maps only)\n",
    "train_images = []\n",
    "for i, normals_path in enumerate(normal_paths):\n",
    "    # Load normals image\n",
    "    normals = cv2.imread(normals_path, cv2.IMREAD_COLOR)\n",
    "    if normals is None or normals.shape[-1] != 3:\n",
    "        print(f\"Error loading normals image: {normals_path}\")\n",
    "        continue\n",
    "    normals = cv2.resize(normals, (SIZE_Y, SIZE_X), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    train_images.append(normals)\n",
    "    print(f\"{i}: {normals_path}\")\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "print(f'\\nTrain images shape: {train_images.shape}')\n",
    "\n",
    "# Load masks\n",
    "train_masks = []\n",
    "mask_paths = sorted(glob.glob(os.path.join(maskDirectory, \"*.png\")))\n",
    "for i, mask_path in enumerate(mask_paths):\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation=cv2.INTER_NEAREST)\n",
    "    train_masks.append(mask)\n",
    "    print(f\"{i}: {mask_path}\")\n",
    "\n",
    "train_masks = np.array(train_masks)\n",
    "print(f'\\nTrain masks shape: {train_masks.shape}')\n",
    "print(f'Unique mask values: {np.unique(train_masks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "# Normalize images\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "train_masks_input = train_masks_encoded_original_shape\n",
    "\n",
    "# Split data: 90% train, 10% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_images, train_masks_input, \n",
    "    test_size=0.1,  # 10% for validation\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} images\")\n",
    "print(f\"Validation set: {X_val.shape[0]} images\")\n",
    "print(f\"Classes in dataset: {np.unique(y_train)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "# Calculate class weights\n",
    "class_labels = np.unique(train_masks_reshaped_encoded)\n",
    "class_weights_balanced = compute_class_weight('balanced', classes=class_labels, y=train_masks_reshaped_encoded)\n",
    "\n",
    "# Use equal weights (set to False for balanced weights)\n",
    "USE_EQUAL_WEIGHTS = True\n",
    "\n",
    "if USE_EQUAL_WEIGHTS:\n",
    "    print(\"\\nUsing EQUAL class weights\")\n",
    "    class_weights = np.ones(NUM_CLASSES)\n",
    "    class_weights_tensor = None\n",
    "else:\n",
    "    print(\"\\nUsing BALANCED class weights\")\n",
    "    class_weights = class_weights_balanced\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(\"\\nClass weights:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"Class {i} ({CLASS_NAMES[i]}): {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "n_classes = NUM_CLASSES\n",
    "\n",
    "print(f\"Model input: {IMG_HEIGHT}x{IMG_WIDTH}x{IMG_CHANNELS}\")\n",
    "print(f\"Output classes: {n_classes}\")\n",
    "\n",
    "# Create model\n",
    "model = MultiUNet(n_channels=IMG_CHANNELS, n_classes=n_classes).to(device)\n",
    "initialize_weights(model)\n",
    "\n",
    "# Loss function\n",
    "class CEDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        \n",
    "        # Dice loss\n",
    "        inputs_soft = F.softmax(inputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0,3,1,2).float()\n",
    "        \n",
    "        dims = (2, 3)\n",
    "        intersection = (inputs_soft * targets_one_hot).sum(dims)\n",
    "        cardinality = inputs_soft.sum(dims) + targets_one_hot.sum(dims)\n",
    "        dice_score = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "        dice_loss = 1. - dice_score.mean()\n",
    "        \n",
    "        return 0.5 * ce_loss + 0.5 * dice_loss\n",
    "\n",
    "criterion = CEDiceLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-7)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# IoU metric\n",
    "def calculate_iou(predictions, labels, n_classes):\n",
    "    ious = []\n",
    "    predictions = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    for cls in range(n_classes):\n",
    "        pred_mask = (predictions == cls)\n",
    "        true_mask = (labels == cls)\n",
    "        \n",
    "        intersection = (pred_mask & true_mask).float().sum()\n",
    "        union = (pred_mask | true_mask).float().sum()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = 1.0 if intersection == 0 else 0.0\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        \n",
    "        ious.append(iou.item() if isinstance(iou, torch.Tensor) else iou)\n",
    "    \n",
    "    return np.mean(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7) Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting training at: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Added shuffle=True for better training\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [],\n",
    "    'iou_metric': [], 'val_iou_metric': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(numEpochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = train_correct = train_total = train_iou = 0.0\n",
    "    \n",
    "    data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}/{numEpochs}') if epoch % 10 == 0 else train_loader\n",
    "    \n",
    "    for data, target in data_iterator:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.numel()\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_iou += calculate_iou(output, target, n_classes)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = val_correct = val_total = val_iou = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.numel()\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "            val_iou += calculate_iou(output, target, n_classes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_train_acc = train_correct / train_total\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "    epoch_train_iou = train_iou / len(train_loader)\n",
    "    epoch_val_iou = val_iou / len(val_loader)\n",
    "    \n",
    "    # Store history\n",
    "    history['loss'].append(epoch_train_loss)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['accuracy'].append(epoch_train_acc)\n",
    "    history['val_accuracy'].append(epoch_val_acc)\n",
    "    history['iou_metric'].append(epoch_train_iou)\n",
    "    history['val_iou_metric'].append(epoch_val_iou)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'\\nEpoch [{epoch+1}/{numEpochs}]')\n",
    "        print(f'Loss: {epoch_train_loss:.4f} -> {epoch_val_loss:.4f}')\n",
    "        print(f'Acc: {epoch_train_acc:.4f} -> {epoch_val_acc:.4f}')\n",
    "        print(f'IoU: {epoch_train_iou:.4f} -> {epoch_val_iou:.4f}')\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'epoch': numEpochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': epoch_train_loss,\n",
    "    'history': history,\n",
    "    'n_classes': n_classes,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'class_colors': CLASS_COLORS,\n",
    "    'img_channels': IMG_CHANNELS\n",
    "}, modelTrainedName)\n",
    "\n",
    "print(f'\\nTraining completed at: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Final results - Loss: {history[\"val_loss\"][-1]:.4f}, Acc: {history[\"val_accuracy\"][-1]:.2%}, IoU: {history[\"val_iou_metric\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history['loss'], 'y-', label='Training')\n",
    "plt.plot(epochs, history['val_loss'], 'r-', label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history['accuracy'], 'y-', label='Training')\n",
    "plt.plot(epochs, history['val_accuracy'], 'r-', label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, history['iou_metric'], 'y-', label='Training')\n",
    "plt.plot(epochs, history['val_iou_metric'], 'r-', label='Validation')\n",
    "plt.title('IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (8) Evaluate and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up colormap\n",
    "cmap_seg = ListedColormap(CLASS_COLORS)\n",
    "model.eval()\n",
    "\n",
    "# Visualize predictions on validation set\n",
    "for idx in range(min(5, len(X_val))):\n",
    "    val_img = X_val[idx]\n",
    "    ground_truth = y_val[idx]\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        val_img_tensor = torch.FloatTensor(val_img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        prediction = model(val_img_tensor)\n",
    "        predicted_img = torch.argmax(prediction, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # Convert normal map for display\n",
    "    normals_disp = cv2.cvtColor((val_img * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(normals_disp)\n",
    "    axes[0].set_title(\"Normal Map Input\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    axes[1].imshow(ground_truth, cmap=cmap_seg, vmin=0, vmax=n_classes-1)\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(predicted_img, cmap=cmap_seg, vmin=0, vmax=n_classes-1)\n",
    "    axes[2].set_title(\"Prediction\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    legend_elements = [Patch(facecolor=CLASS_COLORS[i], label=CLASS_NAMES[i]) for i in range(n_classes)]\n",
    "    fig.legend(handles=legend_elements, loc='center right', bbox_to_anchor=(0.98, 0.5))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (9) Save validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "model_folder_name = modelTrainedName.rsplit('.', 1)[0]\n",
    "output_dir = f\"C:/Users/admin/Desktop/{model_folder_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "cmap_seg = ListedColormap(CLASS_COLORS)\n",
    "\n",
    "# Save all validation predictions\n",
    "for idx in range(len(X_val)):\n",
    "    val_img = X_val[idx]\n",
    "    ground_truth = y_val[idx]\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        val_img_tensor = torch.FloatTensor(val_img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        prediction = model(val_img_tensor)\n",
    "        predicted_img = torch.argmax(prediction, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # Convert normal map for display\n",
    "    normals_disp = cv2.cvtColor((val_img * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(normals_disp)\n",
    "    axes[0].set_title(\"Normal Map Input\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    axes[1].imshow(ground_truth, cmap=cmap_seg, vmin=0, vmax=n_classes-1)\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(predicted_img, cmap=cmap_seg, vmin=0, vmax=n_classes-1)\n",
    "    axes[2].set_title(\"Prediction\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    legend_elements = [Patch(facecolor=CLASS_COLORS[i], label=CLASS_NAMES[i]) for i in range(n_classes)]\n",
    "    fig.legend(handles=legend_elements, loc='center right', bbox_to_anchor=(0.98, 0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"val_image_{idx:04d}.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Saved {len(X_val)} validation predictions to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
