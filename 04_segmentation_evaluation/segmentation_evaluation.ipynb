{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Segmentation Evaluation for Archaeological Masonry\n",
    "\n",
    "This notebook evaluates multi-class segmentation masks using IoU and F1 scores.\n",
    "\n",
    "## RGB Color Mapping:\n",
    "- Black (0,0,0) → Background (Class 0)\n",
    "- Blue (0,0,255) → Ashlar (Class 1)\n",
    "- Red (255,0,0) → Polygonal (Class 2)\n",
    "- Yellow (255,255,0) → Quarry Stone (Class 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from typing import Dict, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for multi-class segmentation masks in archaeological masonry analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_names: List[str] = None, class_colors: List[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with class names and colors.\n",
    "        \n",
    "        Args:\n",
    "            class_names: List of class names (e.g., ['Background', 'Ashlar', 'Polygonal', 'Quarry Stone'])\n",
    "            class_colors: List of colors for visualization\n",
    "        \"\"\"\n",
    "        if class_names is None:\n",
    "            # Default class names for masonry segmentation\n",
    "            self.class_names = ['Background', 'Ashlar', 'Polygonal', 'Quarry Stone']\n",
    "        else:\n",
    "            self.class_names = class_names\n",
    "            \n",
    "        self.num_classes = len(self.class_names)\n",
    "        \n",
    "        if class_colors is None:\n",
    "            # Default color scheme matching your RGB values\n",
    "            self.class_colors = ['#000000', '#0000FF', '#FF0000', '#FFFF00']  # Black, Blue, Red, Yellow\n",
    "        else:\n",
    "            self.class_colors = class_colors\n",
    "            \n",
    "        # RGB color mapping for your specific format\n",
    "        self.rgb_to_class = {\n",
    "            (0, 0, 0): 0,       # Black -> Background\n",
    "            (0, 0, 255): 1,     # Blue -> Ashlar\n",
    "            (255, 0, 0): 2,     # Red -> Polygonal\n",
    "            (255, 255, 0): 3    # Yellow -> Quarry Stone\n",
    "        }\n",
    "        \n",
    "        # Create reverse mapping for visualization\n",
    "        self.class_to_rgb = {v: k for k, v in self.rgb_to_class.items()}\n",
    "    \n",
    "    def load_masks(self, gt_path: str, pred_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load ground truth and predicted masks from file paths.\n",
    "        Converts RGB color-coded masks to class indices.\n",
    "        \n",
    "        Args:\n",
    "            gt_path: Path to ground truth mask\n",
    "            pred_path: Path to predicted mask\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (ground_truth_mask, predicted_mask) as numpy arrays with class indices\n",
    "        \"\"\"\n",
    "        # Load images\n",
    "        gt_img = Image.open(gt_path).convert('RGB')\n",
    "        pred_img = Image.open(pred_path).convert('RGB')\n",
    "        \n",
    "        gt_rgb = np.array(gt_img)\n",
    "        pred_rgb = np.array(pred_img)\n",
    "        \n",
    "        # Ensure masks have same shape\n",
    "        assert gt_rgb.shape[:2] == pred_rgb.shape[:2], f\"Mask shapes don't match: {gt_rgb.shape} vs {pred_rgb.shape}\"\n",
    "        \n",
    "        # Convert RGB to class indices\n",
    "        gt_mask = self.rgb_to_class_indices(gt_rgb)\n",
    "        pred_mask = self.rgb_to_class_indices(pred_rgb)\n",
    "        \n",
    "        # Validate class values\n",
    "        unique_gt = np.unique(gt_mask)\n",
    "        unique_pred = np.unique(pred_mask)\n",
    "        \n",
    "        print(f\"Ground truth classes: {unique_gt}\")\n",
    "        print(f\"Predicted classes: {unique_pred}\")\n",
    "        print(f\"Mask shape: {gt_mask.shape}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        print(\"\\nClass distribution in ground truth:\")\n",
    "        for class_idx in unique_gt:\n",
    "            if class_idx < len(self.class_names):\n",
    "                count = np.sum(gt_mask == class_idx)\n",
    "                percentage = (count / gt_mask.size) * 100\n",
    "                print(f\"  {self.class_names[class_idx]}: {count} pixels ({percentage:.2f}%)\")\n",
    "        \n",
    "        return gt_mask, pred_mask\n",
    "    \n",
    "    def rgb_to_class_indices(self, rgb_mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert RGB mask to class indices based on color mapping.\n",
    "        \n",
    "        Args:\n",
    "            rgb_mask: RGB image array (H, W, 3)\n",
    "            \n",
    "        Returns:\n",
    "            Class index array (H, W)\n",
    "        \"\"\"\n",
    "        height, width = rgb_mask.shape[:2]\n",
    "        class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # Convert each RGB color to its corresponding class\n",
    "        for rgb_tuple, class_idx in self.rgb_to_class.items():\n",
    "            # Create mask for pixels matching this RGB value\n",
    "            color_mask = np.all(rgb_mask == rgb_tuple, axis=2)\n",
    "            class_mask[color_mask] = class_idx\n",
    "        \n",
    "        # Check for unmapped colors (e.g., from anti-aliasing or compression artifacts)\n",
    "        mapped_pixels = np.zeros((height, width), dtype=bool)\n",
    "        for rgb_tuple in self.rgb_to_class.keys():\n",
    "            color_mask = np.all(rgb_mask == rgb_tuple, axis=2)\n",
    "            mapped_pixels |= color_mask\n",
    "        \n",
    "        unmapped_count = np.sum(~mapped_pixels)\n",
    "        if unmapped_count > 0:\n",
    "            print(f\"\\nWarning: {unmapped_count} pixels have unmapped colors!\")\n",
    "            print(\"This might be due to anti-aliasing or compression artifacts.\")\n",
    "            \n",
    "            # Try to map unmapped pixels to nearest class\n",
    "            unmapped_indices = np.where(~mapped_pixels)\n",
    "            for i in range(len(unmapped_indices[0])):\n",
    "                y, x = unmapped_indices[0][i], unmapped_indices[1][i]\n",
    "                pixel_rgb = rgb_mask[y, x]\n",
    "                \n",
    "                # Find nearest color by Euclidean distance\n",
    "                min_dist = float('inf')\n",
    "                nearest_class = 0\n",
    "                \n",
    "                for rgb_tuple, class_idx in self.rgb_to_class.items():\n",
    "                    dist = np.sqrt(np.sum((pixel_rgb.astype(float) - np.array(rgb_tuple).astype(float))**2))\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        nearest_class = class_idx\n",
    "                \n",
    "                class_mask[y, x] = nearest_class\n",
    "            \n",
    "            print(f\"Mapped unmapped pixels to nearest color classes.\")\n",
    "            \n",
    "            # Show unique unmapped colors for debugging\n",
    "            unmapped_pixels = rgb_mask[unmapped_indices]\n",
    "            unique_unmapped = np.unique(unmapped_pixels, axis=0)\n",
    "            if len(unique_unmapped) <= 10:\n",
    "                print(\"\\nUnmapped RGB values found:\")\n",
    "                for color in unique_unmapped:\n",
    "                    print(f\"  RGB{tuple(color)}\")\n",
    "            else:\n",
    "                print(f\"\\nFound {len(unique_unmapped)} unique unmapped colors (showing first 10):\")\n",
    "                for color in unique_unmapped[:10]:\n",
    "                    print(f\"  RGB{tuple(color)}\")\n",
    "        \n",
    "        return class_mask\n",
    "    \n",
    "    def calculate_iou_per_class(self, gt_mask: np.ndarray, pred_mask: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (IoU) for each class.\n",
    "        \n",
    "        Args:\n",
    "            gt_mask: Ground truth mask\n",
    "            pred_mask: Predicted mask\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping class names to IoU scores\n",
    "        \"\"\"\n",
    "        iou_scores = {}\n",
    "        \n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Create binary masks for current class\n",
    "            gt_binary = (gt_mask == class_idx).astype(np.uint8)\n",
    "            pred_binary = (pred_mask == class_idx).astype(np.uint8)\n",
    "            \n",
    "            # Calculate intersection and union\n",
    "            intersection = np.logical_and(gt_binary, pred_binary).sum()\n",
    "            union = np.logical_or(gt_binary, pred_binary).sum()\n",
    "            \n",
    "            # Calculate IoU (handle division by zero)\n",
    "            if union == 0:\n",
    "                iou = 0.0 if intersection == 0 else 1.0\n",
    "            else:\n",
    "                iou = intersection / union\n",
    "            \n",
    "            iou_scores[self.class_names[class_idx]] = iou\n",
    "        \n",
    "        return iou_scores\n",
    "    \n",
    "    def calculate_mean_iou(self, iou_scores: Dict[str, float], ignore_background: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Calculate mean IoU across all classes.\n",
    "        \n",
    "        Args:\n",
    "            iou_scores: Dictionary of per-class IoU scores\n",
    "            ignore_background: Whether to exclude background class from mean calculation\n",
    "            \n",
    "        Returns:\n",
    "            Mean IoU score\n",
    "        \"\"\"\n",
    "        if ignore_background and 'Background' in iou_scores:\n",
    "            # Calculate mean excluding background\n",
    "            relevant_scores = [score for name, score in iou_scores.items() if name != 'Background']\n",
    "        else:\n",
    "            relevant_scores = list(iou_scores.values())\n",
    "        \n",
    "        return np.mean(relevant_scores) if relevant_scores else 0.0\n",
    "    \n",
    "    def calculate_f1_scores(self, gt_mask: np.ndarray, pred_mask: np.ndarray) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Calculate F1 scores (per-class and macro) for segmentation.\n",
    "        \n",
    "        Args:\n",
    "            gt_mask: Ground truth mask\n",
    "            pred_mask: Predicted mask\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing per-class F1 scores and macro F1\n",
    "        \"\"\"\n",
    "        # Flatten masks for sklearn metrics\n",
    "        gt_flat = gt_mask.flatten()\n",
    "        pred_flat = pred_mask.flatten()\n",
    "        \n",
    "        # Get classification report\n",
    "        report = classification_report(\n",
    "            gt_flat, \n",
    "            pred_flat, \n",
    "            labels=list(range(self.num_classes)),\n",
    "            target_names=self.class_names,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Extract F1 scores\n",
    "        f1_scores = {}\n",
    "        for class_name in self.class_names:\n",
    "            if class_name in report:\n",
    "                f1_scores[class_name] = {\n",
    "                    'precision': report[class_name]['precision'],\n",
    "                    'recall': report[class_name]['recall'],\n",
    "                    'f1-score': report[class_name]['f1-score'],\n",
    "                    'support': report[class_name]['support']\n",
    "                }\n",
    "        \n",
    "        # Add macro and weighted averages\n",
    "        f1_scores['macro_avg'] = report['macro avg']['f1-score']\n",
    "        f1_scores['weighted_avg'] = report['weighted avg']['f1-score']\n",
    "        \n",
    "        return f1_scores\n",
    "    \n",
    "    def calculate_confusion_matrix(self, gt_mask: np.ndarray, pred_mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate confusion matrix for segmentation.\n",
    "        \n",
    "        Args:\n",
    "            gt_mask: Ground truth mask\n",
    "            pred_mask: Predicted mask\n",
    "            \n",
    "        Returns:\n",
    "            Confusion matrix\n",
    "        \"\"\"\n",
    "        gt_flat = gt_mask.flatten()\n",
    "        pred_flat = pred_mask.flatten()\n",
    "        \n",
    "        return confusion_matrix(gt_flat, pred_flat, labels=list(range(self.num_classes)))\n",
    "    \n",
    "    def visualize_masks(self, gt_mask: np.ndarray, pred_mask: np.ndarray, \n",
    "                       iou_scores: Dict[str, float], save_path: str = None):\n",
    "        \"\"\"\n",
    "        Visualize ground truth and predicted masks side by side with IoU scores.\n",
    "        \n",
    "        Args:\n",
    "            gt_mask: Ground truth mask\n",
    "            pred_mask: Predicted mask\n",
    "            iou_scores: Dictionary of IoU scores per class\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Create custom colormap matching the RGB scheme\n",
    "        cmap = ListedColormap(self.class_colors[:self.num_classes])\n",
    "        \n",
    "        # Plot ground truth\n",
    "        im1 = axes[0, 0].imshow(gt_mask, cmap=cmap, vmin=0, vmax=self.num_classes-1)\n",
    "        axes[0, 0].set_title('Ground Truth Mask', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Plot prediction\n",
    "        im2 = axes[0, 1].imshow(pred_mask, cmap=cmap, vmin=0, vmax=self.num_classes-1)\n",
    "        axes[0, 1].set_title('Predicted Mask', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Create colorbar with proper labels\n",
    "        cbar_ax = fig.add_axes([0.92, 0.55, 0.02, 0.35])\n",
    "        cbar = plt.colorbar(im1, cax=cbar_ax)\n",
    "        cbar.set_ticks(np.arange(self.num_classes))\n",
    "        cbar.set_ticklabels(self.class_names)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        # Plot difference map\n",
    "        diff_mask = (gt_mask != pred_mask).astype(np.uint8)\n",
    "        axes[1, 0].imshow(diff_mask, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "        axes[1, 0].set_title('Difference Map (Red = Mismatch)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Add colorbar for difference map\n",
    "        diff_pixels = np.sum(diff_mask)\n",
    "        total_pixels = diff_mask.size\n",
    "        error_rate = (diff_pixels / total_pixels) * 100\n",
    "        axes[1, 0].text(0.5, -0.1, f'Error Rate: {error_rate:.2f}% ({diff_pixels:,} pixels)', \n",
    "                       transform=axes[1, 0].transAxes, ha='center', fontsize=10)\n",
    "        \n",
    "        # Plot IoU scores as bar chart with matching colors\n",
    "        classes = list(iou_scores.keys())\n",
    "        scores = list(iou_scores.values())\n",
    "        \n",
    "        # Use the actual colors for each class\n",
    "        bar_colors = []\n",
    "        for class_name in classes:\n",
    "            if class_name in self.class_names:\n",
    "                idx = self.class_names.index(class_name)\n",
    "                bar_colors.append(self.class_colors[idx])\n",
    "            else:\n",
    "                bar_colors.append('#808080')  # Gray for unknown classes\n",
    "        \n",
    "        bars = axes[1, 1].bar(classes, scores, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
    "        axes[1, 1].set_ylabel('IoU Score', fontsize=12)\n",
    "        axes[1, 1].set_title('IoU Scores by Class', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylim(0, 1.1)\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                          f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm: np.ndarray, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix as heatmap.\n",
    "        \n",
    "        Args:\n",
    "            cm: Confusion matrix\n",
    "            save_path: Optional path to save the figure\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaN with 0\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(cm_normalized, annot=cm, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names,\n",
    "                   cbar_kws={'label': 'Normalized Count'})\n",
    "        \n",
    "        plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def generate_report(self, gt_mask: np.ndarray, pred_mask: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive evaluation report.\n",
    "        \n",
    "        Args:\n",
    "            gt_mask: Ground truth mask\n",
    "            pred_mask: Predicted mask\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Calculate all metrics\n",
    "        iou_scores = self.calculate_iou_per_class(gt_mask, pred_mask)\n",
    "        mean_iou = self.calculate_mean_iou(iou_scores, ignore_background=True)\n",
    "        mean_iou_all = self.calculate_mean_iou(iou_scores, ignore_background=False)\n",
    "        f1_scores = self.calculate_f1_scores(gt_mask, pred_mask)\n",
    "        \n",
    "        # Create report dataframe\n",
    "        report_data = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            row = {\n",
    "                'Class': class_name,\n",
    "                'IoU': iou_scores.get(class_name, 0.0),\n",
    "                'Precision': f1_scores.get(class_name, {}).get('precision', 0.0),\n",
    "                'Recall': f1_scores.get(class_name, {}).get('recall', 0.0),\n",
    "                'F1-Score': f1_scores.get(class_name, {}).get('f1-score', 0.0),\n",
    "                'Support': f1_scores.get(class_name, {}).get('support', 0)\n",
    "            }\n",
    "            report_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(report_data)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        summary_df = pd.DataFrame([\n",
    "            {\n",
    "                'Class': 'Mean (excl. background)',\n",
    "                'IoU': mean_iou,\n",
    "                'Precision': '-',\n",
    "                'Recall': '-',\n",
    "                'F1-Score': f1_scores['macro_avg'],\n",
    "                'Support': '-'\n",
    "            },\n",
    "            {\n",
    "                'Class': 'Mean (all classes)',\n",
    "                'IoU': mean_iou_all,\n",
    "                'Precision': '-',\n",
    "                'Recall': '-',\n",
    "                'F1-Score': f1_scores['weighted_avg'],\n",
    "                'Support': '-'\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        df = pd.concat([df, summary_df], ignore_index=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def evaluate_segmentation(gt_path: str, pred_path: str, \n",
    "                         class_names: List[str] = None,\n",
    "                         save_visualizations: bool = True,\n",
    "                         output_dir: str = './evaluation_results/',\n",
    "                         show_rgb_preview: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to evaluate segmentation masks.\n",
    "    \n",
    "    This function handles RGB masks with the following color mapping:\n",
    "    - Black (0,0,0)     -> Background (Class 0)\n",
    "    - Blue (0,0,255)    -> Ashlar (Class 1)\n",
    "    - Red (255,0,0)     -> Polygonal (Class 2)\n",
    "    - Yellow (255,255,0) -> Quarry Stone (Class 3)\n",
    "    \n",
    "    The function automatically converts RGB colors to class indices and handles\n",
    "    edge cases like anti-aliasing or compression artifacts by mapping unmapped\n",
    "    pixels to the nearest valid color class.\n",
    "    \n",
    "    Args:\n",
    "        gt_path: Path to ground truth RGB mask (PNG format)\n",
    "        pred_path: Path to predicted RGB mask (PNG format)\n",
    "        class_names: Optional list of class names\n",
    "        save_visualizations: Whether to save visualization figures\n",
    "        output_dir: Directory to save outputs\n",
    "        show_rgb_preview: Whether to show the original RGB masks\n",
    "    \"\"\"\n",
    "    # Create evaluator\n",
    "    evaluator = SegmentationEvaluator(class_names=class_names)\n",
    "    \n",
    "    # Show RGB preview if requested\n",
    "    if show_rgb_preview:\n",
    "        print(\"Loading RGB masks for preview...\")\n",
    "        gt_rgb = np.array(Image.open(gt_path).convert('RGB'))\n",
    "        pred_rgb = np.array(Image.open(pred_path).convert('RGB'))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ax1.imshow(gt_rgb)\n",
    "        ax1.set_title('Ground Truth RGB Mask', fontsize=14, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(pred_rgb)\n",
    "        ax2.set_title('Predicted RGB Mask', fontsize=14, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Load masks\n",
    "    print(\"\\nLoading and converting RGB masks to class indices...\")\n",
    "    gt_mask, pred_mask = evaluator.load_masks(gt_path, pred_path)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    iou_scores = evaluator.calculate_iou_per_class(gt_mask, pred_mask)\n",
    "    mean_iou = evaluator.calculate_mean_iou(iou_scores, ignore_background=True)\n",
    "    mean_iou_all = evaluator.calculate_mean_iou(iou_scores, ignore_background=False)\n",
    "    f1_scores = evaluator.calculate_f1_scores(gt_mask, pred_mask)\n",
    "    cm = evaluator.calculate_confusion_matrix(gt_mask, pred_mask)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPer-Class IoU Scores:\")\n",
    "    print(\"-\"*30)\n",
    "    for class_name, iou in iou_scores.items():\n",
    "        print(f\"{class_name:20s}: {iou:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMean IoU (excluding background): {mean_iou:.4f}\")\n",
    "    print(f\"Mean IoU (all classes): {mean_iou_all:.4f}\")\n",
    "    \n",
    "    print(\"\\n\\nPer-Class F1 Scores:\")\n",
    "    print(\"-\"*30)\n",
    "    for class_name in evaluator.class_names:\n",
    "        if class_name in f1_scores:\n",
    "            metrics = f1_scores[class_name]\n",
    "            print(f\"\\n{class_name}:\")\n",
    "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"  F1-Score:  {metrics['f1-score']:.4f}\")\n",
    "            print(f\"  Support:   {metrics['support']}\")\n",
    "    \n",
    "    print(f\"\\n\\nMacro F1-Score: {f1_scores['macro_avg']:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1_scores['weighted_avg']:.4f}\")\n",
    "    \n",
    "    # Generate report\n",
    "    report_df = evaluator.generate_report(gt_mask, pred_mask)\n",
    "    print(\"\\n\\nSummary Report:\")\n",
    "    print(\"-\"*60)\n",
    "    print(report_df.to_string(index=False))\n",
    "    \n",
    "    # Visualizations\n",
    "    if save_visualizations:\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Mask comparison\n",
    "        evaluator.visualize_masks(\n",
    "            gt_mask, pred_mask, iou_scores,\n",
    "            save_path=os.path.join(output_dir, 'mask_comparison.png')\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        evaluator.plot_confusion_matrix(\n",
    "            cm,\n",
    "            save_path=os.path.join(output_dir, 'confusion_matrix.png')\n",
    "        )\n",
    "        \n",
    "        # Save report as CSV\n",
    "        report_df.to_csv(os.path.join(output_dir, 'evaluation_report.csv'), index=False)\n",
    "        print(f\"\\n\\nResults saved to: {output_dir}\")\n",
    "    \n",
    "    return report_df, iou_scores, f1_scores, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "Update the paths below to point to your RGB PNG masks and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN EXECUTION CELL\n",
    "# ============================================================\n",
    "\n",
    "# Define your paths here\n",
    "GT_MASK_PATH =\"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/testing/02_masks/Kaunos_Isodom_png-ortho.png\"  # Update this path\n",
    "PRED_MASK_PATH = \"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/testing/05_outputs/Kaunos_Isodom_png-ortho_RAW_combined.png\"   # Update this path\n",
    "\n",
    "# Define your class names (matching your color scheme)\n",
    "CLASS_NAMES = ['Background', 'Ashlar', 'Polygonal', 'Quarry Stone']\n",
    "\n",
    "# Run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # ==== QUICK START: Update these paths and run! ====\n",
    "    results = evaluate_segmentation(\n",
    "        gt_path=\"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/testing/02_masks/Kaunos_Isodom_png-ortho.png\",\n",
    "        pred_path=\"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/testing/05_outputs/Kaunos_Isodom_png-ortho_RAW_combined.png\",\n",
    "        class_names=CLASS_NAMES,\n",
    "        save_visualizations=True,\n",
    "        output_dir=\"C:/Users/admin/Desktop/AWS_TRAINING/2025-08-10_4classEX/testing/06_evaluation\",\n",
    "        show_rgb_preview=True  # Shows original RGB masks before conversion\n",
    ")\n",
    "    \n",
    "    # Save temporary RGB images for demonstration\n",
    "    from PIL import Image\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        gt_path_temp = os.path.join(temp_dir, 'gt_rgb.png')\n",
    "        pred_path_temp = os.path.join(temp_dir, 'pred_rgb.png')\n",
    "        \n",
    "        Image.fromarray(gt_rgb).save(gt_path_temp)\n",
    "        Image.fromarray(pred_rgb).save(pred_path_temp)\n",
    "        \n",
    "        # Create evaluator\n",
    "        evaluator = SegmentationEvaluator(class_names=CLASS_NAMES)\n",
    "        \n",
    "        # Load and convert RGB masks\n",
    "        print(\"Loading RGB masks and converting to class indices...\")\n",
    "        gt_mask, pred_mask = evaluator.load_masks(gt_path_temp, pred_path_temp)\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        iou_scores = evaluator.calculate_iou_per_class(gt_mask, pred_mask)\n",
    "        mean_iou = evaluator.calculate_mean_iou(iou_scores, ignore_background=True)\n",
    "        f1_scores = evaluator.calculate_f1_scores(gt_mask, pred_mask)\n",
    "        cm = evaluator.calculate_confusion_matrix(gt_mask, pred_mask)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        evaluator.visualize_masks(gt_mask, pred_mask, iou_scores)\n",
    "        evaluator.plot_confusion_matrix(cm)\n",
    "        \n",
    "        # Generate report\n",
    "        report = evaluator.generate_report(gt_mask, pred_mask)\n",
    "        print(\"/nExample Evaluation Report:\")\n",
    "        print(report.to_string(index=False))\n",
    "    \n",
    "    print(\"/n\" + \"=\"*60)\n",
    "    print(\"RGB COLOR MAPPING:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Black (0,0,0)     -> Background (Class 0)\")\n",
    "    print(\"Blue (0,0,255)    -> Ashlar (Class 1)\")\n",
    "    print(\"Red (255,0,0)     -> Polygonal (Class 2)\")\n",
    "    print(\"Yellow (255,255,0) -> Quarry Stone (Class 3)\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
